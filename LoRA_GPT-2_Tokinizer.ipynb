{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parameter-Efficient Fine-Tuning GPT-2 Model with LoRA\n",
    "\n",
    "This notebook demonstrates how to use **LoRA (Low-Rank Adaptation)** to fine-tune a pretrained GPT-2 model efficiently. LoRA reduces memory usage and computational costs by freezing most model parameters and training small, low-rank matrices.\n",
    "\n",
    "We will:\n",
    "- Load a pretrained GPT-2 model.\n",
    "- Apply LoRA configuration to the model.\n",
    "- Tokenize input text.\n",
    "- Run a forward pass to verify that the setup works correctly."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Install Required Libraries\n",
    "\n",
    "We will use Hugging Face's `transformers` library for loading GPT-2 and `peft` for applying LoRA."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee3a928e",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install transformers peft"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Load Pretrained GPT-2 Model and Tokenizer\n",
    "\n",
    "Here, we load the pretrained GPT-2 model and its tokenizer. This gives us a base model that has already learned general language patterns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPT-2 model and tokenizer loaded successfully!\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "# Load GPT-2 model and tokenizer\n",
    "model = AutoModelForCausalLM.from_pretrained(\"gpt2\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n",
    "\n",
    "# Set an existing token (e.g., eos_token) as the padding token (The GPT-2 tokenizer already has an eos_token (End of Sequence) that can serve as a padding token)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "# Print confirmation\n",
    "print(\"GPT-2 model and tokenizer loaded successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Troubleshooting Tip:**\n",
    "- If you encounter an error here, ensure that the `transformers` library is installed correctly. Use `pip list` to verify."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Apply LoRA Configuration\n",
    "\n",
    "**LoRA Configuration:**\n",
    "- `r`: Low-rank dimension (e.g., `8`).\n",
    "- `lora_alpha`: Scaling factor (e.g., `32`).\n",
    "- `target_modules`: Specific layers in GPT-2 where LoRA will be applied (e.g., `c_attn`, `c_proj`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LoRA applied successfully!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ahb/Documents/projects/Udacity Author/Micro Lesson/Exercise/udacity_micro_lesson/.venv/lib/python3.12/site-packages/peft/tuners/lora/layer.py:1264: UserWarning: fan_in_fan_out is set to False but the target module is `Conv1D`. Setting fan_in_fan_out to True.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from peft import LoraConfig, get_peft_model\n",
    "\n",
    "# Define LoRA configuration\n",
    "lora_config = LoraConfig(\n",
    "    r=8,            # Low-rank dimension\n",
    "    lora_alpha=32,  # Scaling factor\n",
    "    target_modules=[\"c_attn\", \"c_proj\"],  # Target attention layers in GPT-2\n",
    "    lora_dropout=0.1,\n",
    ") \n",
    "\n",
    "# Apply LoRA to the model\n",
    "model = get_peft_model(model, lora_config)\n",
    "\n",
    "# Print confirmation\n",
    "print(\"LoRA applied successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Common Mistake:** Forgetting to specify the correct target modules (`c_attn`, `c_proj`).\n",
    "- *Solution:* Double-check the layer names in your modelâ€™s architecture if you encounter an error."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 4: Tokenize Input Text\n",
    "\n",
    "**Explanation:** Tokenization converts plain text into numerical IDs that the model can process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input text tokenized successfully!\n"
     ]
    }
   ],
   "source": [
    "# Tokenize a single input text\n",
    "\n",
    "input_text = \"Once upon a time in a galaxy far away\"\n",
    "inputs = tokenizer(input_text, return_tensors=\"pt\").input_ids\n",
    "\n",
    "# Print confirmation\n",
    "print(\"Input text tokenized successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Troubleshooting Tip:** If you encounter a shape mismatch error later, ensure that your tokenized inputs include a batch dimension (e.g., use `.unsqueeze(0)` if needed)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 5: Run a Forward Pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Forward pass completed!\n"
     ]
    }
   ],
   "source": [
    "# Run a forward pass through the model\n",
    "outputs = model(inputs)\n",
    "print(\"Forward pass completed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Common Mistake:** Forgetting to move inputs or models to the appropriate device (e.g., GPU or CPU).\n",
    "- Solution: Ensure both are on compatible devices using `.to(device)`."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
