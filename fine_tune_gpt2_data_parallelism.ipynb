
# Fine-Tuning GPT-2 with Data Parallelism

This notebook demonstrates how to fine-tune a pretrained GPT-2 model using PyTorch's `DataParallel` to distribute training across multiple GPUs.

## Prerequisites
- Python 3.x
- PyTorch
- Hugging Face Transformers library
- A dataset for fine-tuning (e.g., WikiText-2)

## Steps
1. Install required libraries.
2. Load the pretrained GPT-2 model and tokenizer.
3. Tokenize the dataset.
4. Use `DataParallel` for distributed training.
5. Fine-tune the model.
"""),
    nbf.new_code_cell("!pip install transformers datasets"),
    nbf.new_code_cell("""from transformers import AutoModelForCausalLM, AutoTokenizer
from datasets import load_dataset
import torch
from torch.nn.parallel import DataParallel

# Load pretrained GPT-2 model and tokenizer
model = AutoModelForCausalLM.from_pretrained('gpt2')
tokenizer = AutoTokenizer.from_pretrained('gpt2')

# Load a small dataset (e.g., WikiText-2)
dataset = load_dataset('wikitext', 'wikitext-2-v1', split='train[:1%]')

# Tokenize the dataset
def tokenize_function(examples):
    return tokenizer(examples['text'], padding='max_length', truncation=True, max_length=128)

tokenized_dataset = dataset.map(tokenize_function, batched=True)

# Move model to GPU(s) and wrap with DataParallel
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
model = model.to(device)
if torch.cuda.device_count() > 1:
    print(f"Using {torch.cuda.device_count()} GPUs!")
    model = DataParallel(model)

# Dummy training loop
input_ids = torch.tensor(tokenized_dataset['input_ids'][:8]).to(device)  # Batch size of 8
labels = input_ids.clone()

optimizer = torch.optim.AdamW(model.parameters(), lr=5e-5)

# Forward pass
outputs = model(input_ids, labels=labels)
loss = outputs.loss

# Backward pass and optimization step
loss.backward()
optimizer.step()

print("Training step completed!")"""),
    nbf.new_markdown_cell("""## Next Steps

After completing this notebook:
1. Experiment with different datasets.
2. Adjust hyperparameters like learning rate and batch size.
3. Use larger datasets for more meaningful fine-tuning results."""
